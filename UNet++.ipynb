{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "acc0e133",
      "metadata": {
        "id": "acc0e133"
      },
      "source": [
        "# Libraries Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "613f8398",
      "metadata": {
        "id": "613f8398",
        "outputId": "b0d5f268-f093-4456-d368-a42484bbde09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.13.0+cu117\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from skimage import io, color #Scikit-Image\n",
        "from PIL import Image # Pillow\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch # Will work on using PyTorch here later\n",
        "from torch.utils.data  import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "DTdcwEogC-Qv"
      },
      "id": "DTdcwEogC-Qv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b3a05c7b",
      "metadata": {
        "id": "b3a05c7b"
      },
      "source": [
        "# Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parent_folder = '/content/gdrive/MyDrive/HC18'\n",
        "data_folder = '/content/gdrive/MyDrive/HC18/data'"
      ],
      "metadata": {
        "id": "axz1-UmsDBMW"
      },
      "id": "axz1-UmsDBMW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "730fa105",
      "metadata": {
        "id": "730fa105"
      },
      "outputs": [],
      "source": [
        "class HC18(Dataset):\n",
        "    def __init__(self, train = True, transformX = None, transformY = None):\n",
        "        self.pixel_file = pd.read_csv(os.path.join(data_folder, 'training_set_pixel_size_and_HC.csv'))\n",
        "        self.transformX = transformX\n",
        "        self.transformY = transformY\n",
        "        self.train = train\n",
        "        self.train_data, self.validation_data = train_test_split(self.pixel_file, test_size = validation_set_size, random_state = 5)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return len(self.train_data)\n",
        "        return len(self.validation_data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        if self.train:\n",
        "            imx_name = os.path.join(data_folder, 'training_set', self.train_data.iloc[index, 0])\n",
        "            imy_name = os.path.join(data_folder, 'training_set', self.train_data.iloc[index, 0].replace('.png','_Annotation.png'))\n",
        "        else:\n",
        "            imx_name = os.path.join(data_folder, 'training_set', self.validation_data.iloc[index, 0])\n",
        "            imy_name = os.path.join(data_folder, 'training_set', self.validation_data.iloc[index, 0].replace('.png','_Annotation.png'))\n",
        "        imx = Image.open(imx_name)\n",
        "        imy = Image.open(imy_name).convert('L')\n",
        "        \n",
        "        ## tried some data augmentation techniques\n",
        "        if self.train:\n",
        "          # Random horizontal flipping\n",
        "          if random.random() > 0.5:\n",
        "              imx = TF.hflip(imx)\n",
        "              imy = TF.hflip(imy)\n",
        "\n",
        "          # Random vertical flipping\n",
        "          if random.random() > 0.5:\n",
        "              imx = TF.vflip(imx)\n",
        "              imy = TF.vflip(imy)\n",
        "\n",
        "          # Random rotation\n",
        "          if random.random() > 0.8:\n",
        "            angle = random.choice([-30, -90, -60, -45 -15, 0, 15, 30, 45, 60, 90])\n",
        "            imx = TF.rotate(imx, angle)\n",
        "            imy = TF.rotate(imy, angle)\n",
        "        \n",
        "        # We will use resize, tensorlize, and normalize in the following cell\n",
        "        if self.transformX :\n",
        "            imx = self.transformX(imx)\n",
        "            imy = self.transformY(imy)\n",
        "        \n",
        "        sample = {'image': imx, 'annotation': imy}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40487d07",
      "metadata": {
        "id": "40487d07"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(parent_folder, 'train_data.pickle'), 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "with open(os.path.join(parent_folder, 'validation_data.pickle'), 'rb') as f:\n",
        "    validation_data = pickle.load(f)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(dataset = train_data, batch_size = 2)\n",
        "validation_loader = DataLoader(dataset = validation_data, batch_size = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eb6fe4b",
      "metadata": {
        "id": "7eb6fe4b"
      },
      "source": [
        "# Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de2da6cd",
      "metadata": {
        "id": "de2da6cd"
      },
      "source": [
        "# Unet++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b76972",
      "metadata": {
        "id": "a8b76972"
      },
      "outputs": [],
      "source": [
        "from torch.nn import init\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_xavier(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.xavier_normal_(m.weight.data, gain=1)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.xavier_normal_(m.weight.data, gain=1)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_kaiming(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_orthogonal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.orthogonal_(m.weight.data, gain=1)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.orthogonal_(m.weight.data, gain=1)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def init_weights(net, init_type='normal'):\n",
        "    #print('initialization method [%s]' % init_type)\n",
        "    if init_type == 'normal':\n",
        "        net.apply(weights_init_normal)\n",
        "    elif init_type == 'xavier':\n",
        "        net.apply(weights_init_xavier)\n",
        "    elif init_type == 'kaiming':\n",
        "        net.apply(weights_init_kaiming)\n",
        "    elif init_type == 'orthogonal':\n",
        "        net.apply(weights_init_orthogonal)\n",
        "    else:\n",
        "        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a02cec2",
      "metadata": {
        "id": "3a02cec2"
      },
      "outputs": [],
      "source": [
        "class unetConv2(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_batchnorm, n=2, ks=3, stride=1, padding=1):\n",
        "        super(unetConv2, self).__init__()\n",
        "        self.n = n\n",
        "        self.ks = ks\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        s = stride\n",
        "        p = padding\n",
        "        \n",
        "        if is_batchnorm:\n",
        "            for i in range(1, n + 1):\n",
        "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
        "                                     nn.BatchNorm2d(out_size),\n",
        "                                     nn.ReLU(inplace=True), )\n",
        "                setattr(self, 'conv%d' % i, conv)\n",
        "                in_size = out_size\n",
        "\n",
        "        else:\n",
        "            for i in range(1, n + 1):\n",
        "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
        "                                     nn.ReLU(inplace=True), )\n",
        "                setattr(self, 'conv%d' % i, conv)\n",
        "                in_size = out_size\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for i in range(1, self.n + 1):\n",
        "            conv = getattr(self, 'conv%d' % i)\n",
        "            x = conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class unetUp(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_deconv, n_concat=2):\n",
        "        super(unetUp, self).__init__()\n",
        "        # self.conv = unetConv2(in_size + (n_concat - 2) * out_size, out_size, False)\n",
        "        self.conv = unetConv2(out_size*2, out_size, False)\n",
        "        if is_deconv:\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1)\n",
        "        else:\n",
        "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            if m.__class__.__name__.find('unetConv2') != -1: continue\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs0, *input):\n",
        "        # print(self.n_concat)\n",
        "        # print(input)\n",
        "        outputs0 = self.up(inputs0)\n",
        "        for i in range(len(input)):\n",
        "            outputs0 = torch.cat([outputs0, input[i]], 1)\n",
        "        return self.conv(outputs0)\n",
        "    \n",
        "class unetUp_origin(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_deconv, n_concat=2):\n",
        "        super(unetUp_origin, self).__init__()\n",
        "        # self.conv = unetConv2(out_size*2, out_size, False)\n",
        "        if is_deconv:\n",
        "            self.conv = unetConv2(in_size + (n_concat - 2) * out_size, out_size, False)\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
        "        else:\n",
        "            self.conv = unetConv2(in_size + (n_concat - 2) * out_size, out_size, False)\n",
        "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            if m.__class__.__name__.find('unetConv2') != -1: continue\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs0, *input):\n",
        "        x1 = self.up(inputs0)\n",
        "        # input is CHW\n",
        "        for i in range(len(input)):\n",
        "            try:\n",
        "                x1 = torch.cat((x1,input[i]),1)\n",
        "            except:\n",
        "                x1 = torch.cat((F.pad(x1,(1,0,1,0)),input[i]),1)\n",
        "        return self.conv(x1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10187bb2",
      "metadata": {
        "id": "10187bb2"
      },
      "outputs": [],
      "source": [
        "class UNet_2Plus(nn.Module):\n",
        "    def __init__(self, in_channels=1, n_classes=1, feature_scale=4, is_deconv=True, is_batchnorm=True, is_ds=True):\n",
        "        super(UNet_2Plus, self).__init__()\n",
        "        self.is_deconv = is_deconv\n",
        "        self.in_channels = in_channels\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "        self.is_ds = is_ds\n",
        "        self.feature_scale = feature_scale\n",
        "\n",
        "        filters = [64, 128, 256, 512, 1024]\n",
        "\n",
        "        # downsampling\n",
        "        self.conv00 = unetConv2(self.in_channels, filters[0], self.is_batchnorm)\n",
        "        self.maxpool0 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv10 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv20 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv30 = unetConv2(filters[2], filters[3], self.is_batchnorm)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv40 = unetConv2(filters[3], filters[4], self.is_batchnorm)\n",
        "\n",
        "\n",
        "        # upsampling\n",
        "        self.up_concat01 = unetUp_origin(filters[1], filters[0], self.is_deconv)\n",
        "        self.up_concat11 = unetUp_origin(filters[2], filters[1], self.is_deconv)\n",
        "        self.up_concat21 = unetUp_origin(filters[3], filters[2], self.is_deconv)\n",
        "        self.up_concat31 = unetUp_origin(filters[4], filters[3], self.is_deconv)\n",
        "\n",
        "        self.up_concat02 = unetUp_origin(filters[1], filters[0], self.is_deconv, 3)\n",
        "        self.up_concat12 = unetUp_origin(filters[2], filters[1], self.is_deconv, 3)\n",
        "        self.up_concat22 = unetUp_origin(filters[3], filters[2], self.is_deconv, 3)\n",
        "\n",
        "        self.up_concat03 = unetUp_origin(filters[1], filters[0], self.is_deconv, 4)\n",
        "        self.up_concat13 = unetUp_origin(filters[2], filters[1], self.is_deconv, 4)\n",
        "\n",
        "        self.up_concat04 = unetUp_origin(filters[1], filters[0], self.is_deconv, 5)\n",
        "\n",
        "        # final conv (without any concat)\n",
        "        self.final_1 = nn.Conv2d(filters[0], n_classes, 1)\n",
        "        self.final_2 = nn.Conv2d(filters[0], n_classes, 1)\n",
        "        self.final_3 = nn.Conv2d(filters[0], n_classes, 1)\n",
        "        self.final_4 = nn.Conv2d(filters[0], n_classes, 1)\n",
        "\n",
        "        # initialise weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init_weights(m, init_type='kaiming')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # column : 0\n",
        "        X_00 = self.conv00(inputs)\n",
        "        maxpool0 = self.maxpool0(X_00)\n",
        "        X_10 = self.conv10(maxpool0)\n",
        "        maxpool1 = self.maxpool1(X_10)\n",
        "        X_20 = self.conv20(maxpool1)\n",
        "        maxpool2 = self.maxpool2(X_20)\n",
        "        X_30 = self.conv30(maxpool2)\n",
        "        maxpool3 = self.maxpool3(X_30)\n",
        "        X_40 = self.conv40(maxpool3)\n",
        "\n",
        "        # column : 1\n",
        "        X_01 = self.up_concat01(X_10, X_00)\n",
        "        X_11 = self.up_concat11(X_20, X_10)\n",
        "        X_21 = self.up_concat21(X_30, X_20)\n",
        "        X_31 = self.up_concat31(X_40, X_30)\n",
        "        # column : 2\n",
        "        X_02 = self.up_concat02(X_11, X_00, X_01)\n",
        "        X_12 = self.up_concat12(X_21, X_10, X_11)\n",
        "        X_22 = self.up_concat22(X_31, X_20, X_21)\n",
        "        # column : 3\n",
        "        X_03 = self.up_concat03(X_12, X_00, X_01, X_02)\n",
        "        X_13 = self.up_concat13(X_22, X_10, X_11, X_12)\n",
        "        # column : 4\n",
        "        X_04 = self.up_concat04(X_13, X_00, X_01, X_02, X_03)\n",
        "\n",
        "        # final layer\n",
        "        final_1 = self.final_1(X_01)\n",
        "        final_2 = self.final_2(X_02)\n",
        "        final_3 = self.final_3(X_03)\n",
        "        final_4 = self.final_4(X_04)\n",
        "\n",
        "        final = (final_1 + final_2 + final_3 + final_4) / 4\n",
        "\n",
        "        if self.is_ds:\n",
        "            return F.sigmoid(final)\n",
        "        else:\n",
        "            return F.sigmoid(final_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1faeeffa",
      "metadata": {
        "id": "1faeeffa",
        "outputId": "bb0622f1-99f2-461b-c7c9-7603ecfcce5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Loaded to GPU\n"
          ]
        }
      ],
      "source": [
        "model = UNet_2Plus()\n",
        "model.to('cuda')\n",
        "print(\"Model Loaded to GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f087ee21",
      "metadata": {
        "id": "f087ee21"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9849b274",
      "metadata": {
        "id": "9849b274"
      },
      "outputs": [],
      "source": [
        "# calculates similarity index between predicted and actual segmentation\n",
        "def dice_index(y_pred, y_actual):\n",
        "    smooth = 0.000001\n",
        "    size_of_batch = y_pred.size(0)\n",
        "    \n",
        "    p1 = y_pred.view(size_of_batch, -1)\n",
        "    p2 = y_actual.view(size_of_batch, -1)\n",
        "    \n",
        "    intersection = (p1 * p2).sum()\n",
        "    \n",
        "    dice =  ((2.0 * intersection )+ smooth) / (p1.sum() + p2.sum() + smooth)\n",
        "    #dice.requires_grad = True\n",
        "    \n",
        "    return dice\n",
        "\n",
        "# calculate dice loss which will be later used in loss function calculation\n",
        "def dice_loss(y_predict, y_train): ## to add in bce looss\n",
        "    return 1 -(dice_index(y_predict, y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Only train the model if you want to retrain, screw down to load the pre-trained model"
      ],
      "metadata": {
        "id": "1FudpWLDDJVw"
      },
      "id": "1FudpWLDDJVw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3526d833",
      "metadata": {
        "scrolled": false,
        "id": "3526d833",
        "outputId": "2473167c-a736-41c4-e619-f624296d9ec9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mmfs1/data/licds/.local/lib/python3.9/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  1 Batch:  0 Current Loss:  0.6849687099456787\n",
            "Epoch:  1 Batch:  50 Current Loss:  0.4218757748603821\n",
            "Epoch:  1 Batch:  100 Current Loss:  0.2937900424003601\n",
            "Epoch:  1 Batch:  150 Current Loss:  0.41865459084510803\n",
            "Epoch:  1 Batch:  200 Current Loss:  0.29288041591644287\n",
            "Epoch:  1 Batch:  250 Current Loss:  0.23436874151229858\n",
            "Epoch:  1 Batch:  300 Current Loss:  0.20493990182876587\n",
            "Epoch:  1 Batch:  350 Current Loss:  0.17475487291812897\n",
            "================================================================================\n",
            "Epoch 1 completed\n",
            "Average train loss is 0.31955666963011026: \n",
            "Average validation loss is 0.23209361523389815\n",
            "================================================================================\n",
            "Epoch:  2 Batch:  0 Current Loss:  0.3636965751647949\n",
            "Epoch:  2 Batch:  50 Current Loss:  0.35569965839385986\n",
            "Epoch:  2 Batch:  100 Current Loss:  0.13151085376739502\n",
            "Epoch:  2 Batch:  150 Current Loss:  0.37908387184143066\n",
            "Epoch:  2 Batch:  200 Current Loss:  0.14523077011108398\n",
            "Epoch:  2 Batch:  250 Current Loss:  0.23160696029663086\n",
            "Epoch:  2 Batch:  300 Current Loss:  0.1410256326198578\n",
            "Epoch:  2 Batch:  350 Current Loss:  0.13326768577098846\n",
            "================================================================================\n",
            "Epoch 2 completed\n",
            "Average train loss is 0.2196810577623546: \n",
            "Average validation loss is 0.1629758246988058\n",
            "================================================================================\n",
            "Epoch:  3 Batch:  0 Current Loss:  0.2686358690261841\n",
            "Epoch:  3 Batch:  50 Current Loss:  0.23856958746910095\n",
            "Epoch:  3 Batch:  100 Current Loss:  0.08563397079706192\n",
            "Epoch:  3 Batch:  150 Current Loss:  0.26260507106781006\n",
            "Epoch:  3 Batch:  200 Current Loss:  0.08872190862894058\n",
            "Epoch:  3 Batch:  250 Current Loss:  0.1854933500289917\n",
            "Epoch:  3 Batch:  300 Current Loss:  0.10465820878744125\n",
            "Epoch:  3 Batch:  350 Current Loss:  0.08115015923976898\n",
            "================================================================================\n",
            "Epoch 3 completed\n",
            "Average train loss is 0.1628201641328633: \n",
            "Average validation loss is 0.1087536308914423\n",
            "================================================================================\n",
            "Epoch:  4 Batch:  0 Current Loss:  0.20790767669677734\n",
            "Epoch:  4 Batch:  50 Current Loss:  0.16412079334259033\n",
            "Epoch:  4 Batch:  100 Current Loss:  0.0565149150788784\n",
            "Epoch:  4 Batch:  150 Current Loss:  0.20506665110588074\n",
            "Epoch:  4 Batch:  200 Current Loss:  0.0909370481967926\n",
            "Epoch:  4 Batch:  250 Current Loss:  0.11550448089838028\n",
            "Epoch:  4 Batch:  300 Current Loss:  0.06728096306324005\n",
            "Epoch:  4 Batch:  350 Current Loss:  0.0641167163848877\n",
            "================================================================================\n",
            "Epoch 4 completed\n",
            "Average train loss is 0.12177440376020968: \n",
            "Average validation loss is 0.10880430735647678\n",
            "================================================================================\n",
            "Epoch:  5 Batch:  0 Current Loss:  0.18376322090625763\n",
            "Epoch:  5 Batch:  50 Current Loss:  0.12941017746925354\n",
            "Epoch:  5 Batch:  100 Current Loss:  0.04587404802441597\n",
            "Epoch:  5 Batch:  150 Current Loss:  0.15129399299621582\n",
            "Epoch:  5 Batch:  200 Current Loss:  0.04490020498633385\n",
            "Epoch:  5 Batch:  250 Current Loss:  0.1049685925245285\n",
            "Epoch:  5 Batch:  300 Current Loss:  0.04888896271586418\n",
            "Epoch:  5 Batch:  350 Current Loss:  0.06550171971321106\n",
            "================================================================================\n",
            "Epoch 5 completed\n",
            "Average train loss is 0.10442706428468228: \n",
            "Average validation loss is 0.09172540556639433\n",
            "================================================================================\n",
            "Epoch:  6 Batch:  0 Current Loss:  0.15686234831809998\n",
            "Epoch:  6 Batch:  50 Current Loss:  0.08192523568868637\n",
            "Epoch:  6 Batch:  100 Current Loss:  0.03615890443325043\n",
            "Epoch:  6 Batch:  150 Current Loss:  0.17455853521823883\n",
            "Epoch:  6 Batch:  200 Current Loss:  0.0404600128531456\n",
            "Epoch:  6 Batch:  250 Current Loss:  0.08874126523733139\n",
            "Epoch:  6 Batch:  300 Current Loss:  0.05809357762336731\n",
            "Epoch:  6 Batch:  350 Current Loss:  0.06303206831216812\n",
            "================================================================================\n",
            "Epoch 6 completed\n",
            "Average train loss is 0.09389082061126829: \n",
            "Average validation loss is 0.1009050004556775\n",
            "================================================================================\n",
            "Epoch:  7 Batch:  0 Current Loss:  0.19365927577018738\n",
            "Epoch:  7 Batch:  50 Current Loss:  0.11310316622257233\n",
            "Epoch:  7 Batch:  100 Current Loss:  0.04970335215330124\n",
            "Epoch:  7 Batch:  150 Current Loss:  0.10049368441104889\n",
            "Epoch:  7 Batch:  200 Current Loss:  0.04214799404144287\n",
            "Epoch:  7 Batch:  250 Current Loss:  0.09833789616823196\n",
            "Epoch:  7 Batch:  300 Current Loss:  0.04983175918459892\n",
            "Epoch:  7 Batch:  350 Current Loss:  0.05298919603228569\n",
            "================================================================================\n",
            "Epoch 7 completed\n",
            "Average train loss is 0.0862859671190381: \n",
            "Average validation loss is 0.0843474924378097\n",
            "================================================================================\n",
            "Epoch:  8 Batch:  0 Current Loss:  0.1320212185382843\n",
            "Epoch:  8 Batch:  50 Current Loss:  0.07992628216743469\n",
            "Epoch:  8 Batch:  100 Current Loss:  0.04008599370718002\n",
            "Epoch:  8 Batch:  150 Current Loss:  0.24613124132156372\n",
            "Epoch:  8 Batch:  200 Current Loss:  0.04079592972993851\n",
            "Epoch:  8 Batch:  250 Current Loss:  0.10232041031122208\n",
            "Epoch:  8 Batch:  300 Current Loss:  0.05083933100104332\n",
            "Epoch:  8 Batch:  350 Current Loss:  0.09351006150245667\n",
            "================================================================================\n",
            "Epoch 8 completed\n",
            "Average train loss is 0.08576522401999682: \n",
            "Average validation loss is 0.08619496669620276\n",
            "================================================================================\n",
            "Epoch:  9 Batch:  0 Current Loss:  0.10668390989303589\n",
            "Epoch:  9 Batch:  50 Current Loss:  0.06577044725418091\n",
            "Epoch:  9 Batch:  100 Current Loss:  0.032319992780685425\n",
            "Epoch:  9 Batch:  150 Current Loss:  0.10419546812772751\n",
            "Epoch:  9 Batch:  200 Current Loss:  0.03808600455522537\n",
            "Epoch:  9 Batch:  250 Current Loss:  0.08943337947130203\n",
            "Epoch:  9 Batch:  300 Current Loss:  0.05601812154054642\n",
            "Epoch:  9 Batch:  350 Current Loss:  0.09586750715970993\n",
            "================================================================================\n",
            "Epoch 9 completed\n",
            "Average train loss is 0.07971457339357585: \n",
            "Average validation loss is 0.09980493180453777\n",
            "================================================================================\n",
            "Epoch:  10 Batch:  0 Current Loss:  0.10327814519405365\n",
            "Epoch:  10 Batch:  50 Current Loss:  0.0677497610449791\n",
            "Epoch:  10 Batch:  100 Current Loss:  0.031050963327288628\n",
            "Epoch:  10 Batch:  150 Current Loss:  0.09100145846605301\n",
            "Epoch:  10 Batch:  200 Current Loss:  0.039239272475242615\n",
            "Epoch:  10 Batch:  250 Current Loss:  0.07855424284934998\n",
            "Epoch:  10 Batch:  300 Current Loss:  0.060761768370866776\n",
            "Epoch:  10 Batch:  350 Current Loss:  0.06589899957180023\n",
            "================================================================================\n",
            "Epoch 10 completed\n",
            "Average train loss is 0.07791233690455555: \n",
            "Average validation loss is 0.09985614307224751\n",
            "================================================================================\n",
            "Epoch:  11 Batch:  0 Current Loss:  0.12161192297935486\n",
            "Epoch:  11 Batch:  50 Current Loss:  0.06658761948347092\n",
            "Epoch:  11 Batch:  100 Current Loss:  0.03021412342786789\n",
            "Epoch:  11 Batch:  150 Current Loss:  0.11384408175945282\n",
            "Epoch:  11 Batch:  200 Current Loss:  0.04372221603989601\n",
            "Epoch:  11 Batch:  250 Current Loss:  0.09086047112941742\n",
            "Epoch:  11 Batch:  300 Current Loss:  0.047335896641016006\n",
            "Epoch:  11 Batch:  350 Current Loss:  0.06072697415947914\n",
            "================================================================================\n",
            "Epoch 11 completed\n",
            "Average train loss is 0.07397518123500049: \n",
            "Average validation loss is 0.08221061980351806\n",
            "================================================================================\n",
            "Epoch:  12 Batch:  0 Current Loss:  0.11405152082443237\n",
            "Epoch:  12 Batch:  50 Current Loss:  0.08888782560825348\n",
            "Epoch:  12 Batch:  100 Current Loss:  0.03090466558933258\n",
            "Epoch:  12 Batch:  150 Current Loss:  0.14727580547332764\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  12 Batch:  200 Current Loss:  0.04498247802257538\n",
            "Epoch:  12 Batch:  250 Current Loss:  0.09166416525840759\n",
            "Epoch:  12 Batch:  300 Current Loss:  0.058626215904951096\n",
            "Epoch:  12 Batch:  350 Current Loss:  0.054936978965997696\n",
            "================================================================================\n",
            "Epoch 12 completed\n",
            "Average train loss is 0.07286706156563014: \n",
            "Average validation loss is 0.09801721889525652\n",
            "================================================================================\n",
            "Epoch:  13 Batch:  0 Current Loss:  0.13509078323841095\n",
            "Epoch:  13 Batch:  50 Current Loss:  0.10033158957958221\n",
            "Epoch:  13 Batch:  100 Current Loss:  0.03439243137836456\n",
            "Epoch:  13 Batch:  150 Current Loss:  0.1153562068939209\n",
            "Epoch:  13 Batch:  200 Current Loss:  0.040074434131383896\n",
            "Epoch:  13 Batch:  250 Current Loss:  0.08442524820566177\n",
            "Epoch:  13 Batch:  300 Current Loss:  0.03568305820226669\n",
            "Epoch:  13 Batch:  350 Current Loss:  0.05678839236497879\n",
            "================================================================================\n",
            "Epoch 13 completed\n",
            "Average train loss is 0.07225532432552427: \n",
            "Average validation loss is 0.07827354269102216\n",
            "================================================================================\n",
            "Epoch:  14 Batch:  0 Current Loss:  0.08119191974401474\n",
            "Epoch:  14 Batch:  50 Current Loss:  0.06388536095619202\n",
            "Epoch:  14 Batch:  100 Current Loss:  0.0267043374478817\n",
            "Epoch:  14 Batch:  150 Current Loss:  0.05601249635219574\n",
            "Epoch:  14 Batch:  200 Current Loss:  0.04098068177700043\n",
            "Epoch:  14 Batch:  250 Current Loss:  0.09621114283800125\n",
            "Epoch:  14 Batch:  300 Current Loss:  0.0529060959815979\n",
            "Epoch:  14 Batch:  350 Current Loss:  0.059413764625787735\n",
            "================================================================================\n",
            "Epoch 14 completed\n",
            "Average train loss is 0.06652816826943309: \n",
            "Average validation loss is 0.06395368637517095\n",
            "================================================================================\n",
            "Epoch:  15 Batch:  0 Current Loss:  0.11487466096878052\n",
            "Epoch:  15 Batch:  50 Current Loss:  0.08739134669303894\n",
            "Epoch:  15 Batch:  100 Current Loss:  0.026698540896177292\n",
            "Epoch:  15 Batch:  150 Current Loss:  0.11819028854370117\n",
            "Epoch:  15 Batch:  200 Current Loss:  0.04179341718554497\n",
            "Epoch:  15 Batch:  250 Current Loss:  0.08596508204936981\n",
            "Epoch:  15 Batch:  300 Current Loss:  0.05000963807106018\n",
            "Epoch:  15 Batch:  350 Current Loss:  0.08666565269231796\n",
            "================================================================================\n",
            "Epoch 15 completed\n",
            "Average train loss is 0.06833650753833354: \n",
            "Average validation loss is 0.07947601841762662\n",
            "================================================================================\n",
            "Epoch:  16 Batch:  0 Current Loss:  0.1226019337773323\n",
            "Epoch:  16 Batch:  50 Current Loss:  0.060833461582660675\n",
            "Epoch:  16 Batch:  100 Current Loss:  0.03886168450117111\n",
            "Epoch:  16 Batch:  150 Current Loss:  0.07844053208827972\n",
            "Epoch:  16 Batch:  200 Current Loss:  0.03759445250034332\n",
            "Epoch:  16 Batch:  250 Current Loss:  0.10788130760192871\n",
            "Epoch:  16 Batch:  300 Current Loss:  0.08843734860420227\n",
            "Epoch:  16 Batch:  350 Current Loss:  0.06641785800457001\n",
            "================================================================================\n",
            "Epoch 16 completed\n",
            "Average train loss is 0.0661989361885935: \n",
            "Average validation loss is 0.06286567337810993\n",
            "================================================================================\n",
            "Epoch:  17 Batch:  0 Current Loss:  0.07780034840106964\n",
            "Epoch:  17 Batch:  50 Current Loss:  0.05600930005311966\n",
            "Epoch:  17 Batch:  100 Current Loss:  0.027318540960550308\n",
            "Epoch:  17 Batch:  150 Current Loss:  0.21536897122859955\n",
            "Epoch:  17 Batch:  200 Current Loss:  0.04353495314717293\n",
            "Epoch:  17 Batch:  250 Current Loss:  0.0704539492726326\n",
            "Epoch:  17 Batch:  300 Current Loss:  0.041115231812000275\n",
            "Epoch:  17 Batch:  350 Current Loss:  0.10969526320695877\n",
            "================================================================================\n",
            "Epoch 17 completed\n",
            "Average train loss is 0.0638195982715115: \n",
            "Average validation loss is 0.06612039152532816\n",
            "================================================================================\n",
            "Epoch:  18 Batch:  0 Current Loss:  0.08155324310064316\n",
            "Epoch:  18 Batch:  50 Current Loss:  0.04628990963101387\n",
            "Epoch:  18 Batch:  100 Current Loss:  0.026878798380494118\n",
            "Epoch:  18 Batch:  150 Current Loss:  0.07592472434043884\n",
            "Epoch:  18 Batch:  200 Current Loss:  0.03796502575278282\n",
            "Epoch:  18 Batch:  250 Current Loss:  0.0752507746219635\n",
            "Epoch:  18 Batch:  300 Current Loss:  0.04443107545375824\n",
            "Epoch:  18 Batch:  350 Current Loss:  0.0637224018573761\n",
            "================================================================================\n",
            "Epoch 18 completed\n",
            "Average train loss is 0.059994298596866426: \n",
            "Average validation loss is 0.0671336186490953\n",
            "================================================================================\n",
            "Epoch:  19 Batch:  0 Current Loss:  0.06366215646266937\n",
            "Epoch:  19 Batch:  50 Current Loss:  0.04320244863629341\n",
            "Epoch:  19 Batch:  100 Current Loss:  0.022542953491210938\n",
            "Epoch:  19 Batch:  150 Current Loss:  0.12155663967132568\n",
            "Epoch:  19 Batch:  200 Current Loss:  0.03896866366267204\n",
            "Epoch:  19 Batch:  250 Current Loss:  0.08186112344264984\n",
            "Epoch:  19 Batch:  300 Current Loss:  0.035124294459819794\n",
            "Epoch:  19 Batch:  350 Current Loss:  0.0738152489066124\n",
            "================================================================================\n",
            "Epoch 19 completed\n",
            "Average train loss is 0.06094849755521864: \n",
            "Average validation loss is 0.06438284764066339\n",
            "================================================================================\n",
            "Epoch:  20 Batch:  0 Current Loss:  0.06400257349014282\n",
            "Epoch:  20 Batch:  50 Current Loss:  0.06080375611782074\n",
            "Epoch:  20 Batch:  100 Current Loss:  0.026159025728702545\n",
            "Epoch:  20 Batch:  150 Current Loss:  0.05087192729115486\n",
            "Epoch:  20 Batch:  200 Current Loss:  0.0343068465590477\n",
            "Epoch:  20 Batch:  250 Current Loss:  0.07715854793787003\n",
            "Epoch:  20 Batch:  300 Current Loss:  0.058615490794181824\n",
            "Epoch:  20 Batch:  350 Current Loss:  0.06191463768482208\n",
            "================================================================================\n",
            "Epoch 20 completed\n",
            "Average train loss is 0.05945912781637162: \n",
            "Average validation loss is 0.0744778798893094\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "epochs = 20 \n",
        "\n",
        "train_running_loss_history = []\n",
        "validation_running_loss_history =[]\n",
        "\n",
        "for e in range(epochs):\n",
        "    train_running_loss = 0.0\n",
        "    validation_running_loss = 0.0  \n",
        "    model.train()\n",
        "    for ith_batch, sample_batched in enumerate(train_loader):\n",
        "        X_train = sample_batched['image'].to(\"cuda\")\n",
        "        y_train = sample_batched['annotation'].to(\"cuda\")\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_train)\n",
        "        loss = 0.30 * dice_loss(y_pred, y_train) +  0.70 * criterion(y_pred, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if ith_batch % 50 == 0:\n",
        "            print('Epoch: ', e + 1, 'Batch: ', ith_batch, 'Current Loss: ', loss.item())\n",
        "        train_running_loss += loss.item()\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for ith_batch, sample_batched in enumerate(validation_loader):\n",
        "                X_val = sample_batched['image'].to(\"cuda\")\n",
        "                y_val = sample_batched['annotation'].to(\"cuda\")\n",
        "                y_out = model(X_val)\n",
        "                out_val = (y_out + 0.5).int().float()\n",
        "                val_loss = 0.3 * dice_loss(out_val, y_val)  + 0.7 * criterion(y_out, y_val)\n",
        "                validation_running_loss += val_loss.item()\n",
        "            print(\"================================================================================\")\n",
        "            print(\"Epoch {} completed\".format(e + 1))\n",
        "\n",
        "            train_epoch_loss = train_running_loss / len(train_loader)\n",
        "            validation_epoch_loss = validation_running_loss / len(validation_loader)\n",
        "\n",
        "            print(\"Average train loss is {}: \".format(train_epoch_loss))\n",
        "            print(\"Average validation loss is {}\".format(validation_epoch_loss))\n",
        "            print(\"================================================================================\")\n",
        "            train_running_loss_history.append(train_epoch_loss)\n",
        "            validation_running_loss_history.append(validation_epoch_loss)\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e50f178",
      "metadata": {
        "id": "5e50f178",
        "outputId": "4dca859f-c001-4f2d-ed07-ffd334b26534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x155420059370>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_running_loss_history, label = 'Train Loss')\n",
        "plt.plot(validation_running_loss_history, label = 'Validation Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf76e565",
      "metadata": {
        "id": "bf76e565",
        "outputId": "2912b5ce-552c-4dcc-9d61-82c6439d3463"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 572, 572])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()\n",
        "X_train.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3af1becc",
      "metadata": {
        "id": "3af1becc"
      },
      "outputs": [],
      "source": [
        "torch.save(model, os.path.join(parent_folder, 'savedmodel/UNet++.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You can load a model instead"
      ],
      "metadata": {
        "id": "pMHbvDcsDRDk"
      },
      "id": "pMHbvDcsDRDk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "417f5379",
      "metadata": {
        "id": "417f5379",
        "outputId": "bd9b8b49-d689-4d11-d281-9874ece0497f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "UNet_2Plus(\n",
              "  (conv00): unetConv2(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (maxpool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv10): unetConv2(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv20): unetConv2(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv30): unetConv2(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv40): unetConv2(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (up_concat01): unetUp_origin(\n",
              "    (conv): unetConv2(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (up_concat11): unetUp_origin(\n",
              "    (conv): unetConv2(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (up_concat21): unetUp_origin(\n",
              "    (conv): unetConv2(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (up_concat31): unetUp_origin(\n",
              "    (conv): unetConv2(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (up_concat02): unetUp_origin(\n",
              "    (conv): unetConv2(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (up_concat12): unetUp_origin(\n",
              "    (conv): unetConv2(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (up_concat22): unetUp_origin(\n",
              "    (conv): unetConv2(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (up_concat03): unetUp_origin(\n",
              "    (conv): unetConv2(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (up_concat13): unetUp_origin(\n",
              "    (conv): unetConv2(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (up_concat04): unetUp_origin(\n",
              "    (conv): unetConv2(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  )\n",
              "  (final_1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (final_2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (final_3): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (final_4): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = torch.load(os.path.join(parent_folder, 'savedmodel/UNet++.pt'))\n",
        "model.to('cuda')\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe50d1b",
      "metadata": {
        "id": "abe50d1b"
      },
      "outputs": [],
      "source": [
        "def avg_dice_index(dataloader):\n",
        "    dice = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for ith_batch, sample_batched in enumerate(dataloader):\n",
        "            X_train = sample_batched['image'].to('cuda')\n",
        "            y_train = sample_batched['annotation'].to('cuda')\n",
        "            y_predict = (model(X_train) + 0.5).int().float()\n",
        "            dice += dice_index(y_predict, y_train)\n",
        "    avg_dice = dice / len(dataloader)\n",
        "    return avg_dice.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4558cda4",
      "metadata": {
        "id": "4558cda4",
        "outputId": "e490a37f-4185-45cb-f3cd-2606a3051a23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mmfs1/data/licds/.local/lib/python3.9/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9463185667991638"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "avg_dice_index(validation_loader)    #### Dice index of validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "175c35d2",
      "metadata": {
        "id": "175c35d2",
        "outputId": "30041ff6-c8fb-4c21-9f6c-71cee0bdc537"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9438002705574036"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "avg_dice_index(train_loader)    #### Dice index of validation data"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}