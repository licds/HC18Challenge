{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "acc0e133",
      "metadata": {
        "id": "acc0e133"
      },
      "source": [
        "# Libraries Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "613f8398",
      "metadata": {
        "id": "613f8398",
        "outputId": "921657da-43f6-4544-e69a-c061d7b88e33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.13.0+cu117\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from skimage import io, color #Scikit-Image\n",
        "from PIL import Image # Pillow\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch # Will work on using PyTorch here later\n",
        "from torch.utils.data  import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "jdbYg7QYlnvB"
      },
      "id": "jdbYg7QYlnvB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b3a05c7b",
      "metadata": {
        "id": "b3a05c7b"
      },
      "source": [
        "# Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parent_folder = '/content/gdrive/MyDrive/HC18'\n",
        "data_folder = '/content/gdrive/MyDrive/HC18/data'"
      ],
      "metadata": {
        "id": "k5CpJjJSlnxI"
      },
      "id": "k5CpJjJSlnxI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98272224",
      "metadata": {
        "id": "98272224"
      },
      "outputs": [],
      "source": [
        "class HC18(Dataset):\n",
        "    def __init__(self, train = True, transformX = None, transformY = None):\n",
        "        self.pixel_file = pd.read_csv(os.path.join(data_folder, 'training_set_pixel_size_and_HC.csv'))\n",
        "        self.transformX = transformX\n",
        "        self.transformY = transformY\n",
        "        self.train = train\n",
        "        self.train_data, self.validation_data = train_test_split(self.pixel_file, test_size = validation_set_size, random_state = 5)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return len(self.train_data)\n",
        "        return len(self.validation_data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        if self.train:\n",
        "            imx_name = os.path.join(data_folder, 'training_set', self.train_data.iloc[index, 0])\n",
        "            imy_name = os.path.join(data_folder, 'training_set', self.train_data.iloc[index, 0].replace('.png','_Annotation.png'))\n",
        "        else:\n",
        "            imx_name = os.path.join(data_folder, 'training_set', self.validation_data.iloc[index, 0])\n",
        "            imy_name = os.path.join(data_folder, 'training_set', self.validation_data.iloc[index, 0].replace('.png','_Annotation.png'))\n",
        "        imx = Image.open(imx_name)\n",
        "        imy = Image.open(imy_name).convert('L')\n",
        "        \n",
        "        ## tried some data augmentation techniques\n",
        "        if self.train:\n",
        "          # Random horizontal flipping\n",
        "          if random.random() > 0.5:\n",
        "              imx = TF.hflip(imx)\n",
        "              imy = TF.hflip(imy)\n",
        "\n",
        "          # Random vertical flipping\n",
        "          if random.random() > 0.5:\n",
        "              imx = TF.vflip(imx)\n",
        "              imy = TF.vflip(imy)\n",
        "\n",
        "          # Random rotation\n",
        "          if random.random() > 0.8:\n",
        "            angle = random.choice([-30, -90, -60, -45 -15, 0, 15, 30, 45, 60, 90])\n",
        "            imx = TF.rotate(imx, angle)\n",
        "            imy = TF.rotate(imy, angle)\n",
        "        \n",
        "        # We will use resize, tensorlize, and normalize in the following cell\n",
        "        if self.transformX :\n",
        "            imx = self.transformX(imx)\n",
        "            imy = self.transformY(imy)\n",
        "        \n",
        "        sample = {'image': imx, 'annotation': imy}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfcf418a",
      "metadata": {
        "id": "dfcf418a"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(parent_folder, 'train_data.pickle'), 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "with open(os.path.join(parent_folder, 'validation_data.pickle'), 'rb') as f:\n",
        "    validation_data = pickle.load(f)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(dataset = train_data, batch_size = 2)\n",
        "validation_loader = DataLoader(dataset = validation_data, batch_size = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eb6fe4b",
      "metadata": {
        "id": "7eb6fe4b"
      },
      "source": [
        "# Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5a9be50",
      "metadata": {
        "id": "c5a9be50"
      },
      "outputs": [],
      "source": [
        "from torch.nn import init\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_xavier(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.xavier_normal_(m.weight.data, gain=1)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.xavier_normal_(m.weight.data, gain=1)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_kaiming(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_orthogonal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.orthogonal_(m.weight.data, gain=1)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.orthogonal_(m.weight.data, gain=1)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def init_weights(net, init_type='normal'):\n",
        "    #print('initialization method [%s]' % init_type)\n",
        "    if init_type == 'normal':\n",
        "        net.apply(weights_init_normal)\n",
        "    elif init_type == 'xavier':\n",
        "        net.apply(weights_init_xavier)\n",
        "    elif init_type == 'kaiming':\n",
        "        net.apply(weights_init_kaiming)\n",
        "    elif init_type == 'orthogonal':\n",
        "        net.apply(weights_init_orthogonal)\n",
        "    else:\n",
        "        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96dc1d58",
      "metadata": {
        "id": "96dc1d58"
      },
      "outputs": [],
      "source": [
        "class unetConv2(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_batchnorm, n=2, ks=3, stride=1, padding=1):\n",
        "        super(unetConv2, self).__init__()\n",
        "        self.n = n\n",
        "        self.ks = ks\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        s = stride\n",
        "        p = padding\n",
        "        \n",
        "        if is_batchnorm:\n",
        "            for i in range(1, n + 1):\n",
        "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
        "                                     nn.BatchNorm2d(out_size),\n",
        "                                     nn.ReLU(inplace=True), )\n",
        "                setattr(self, 'conv%d' % i, conv)\n",
        "                in_size = out_size\n",
        "\n",
        "        else:\n",
        "            for i in range(1, n + 1):\n",
        "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
        "                                     nn.ReLU(inplace=True), )\n",
        "                setattr(self, 'conv%d' % i, conv)\n",
        "                in_size = out_size\n",
        "\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for i in range(1, self.n + 1):\n",
        "            conv = getattr(self, 'conv%d' % i)\n",
        "            x = conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class unetUp(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_deconv, n_concat=2):\n",
        "        super(unetUp, self).__init__()\n",
        "        # self.conv = unetConv2(in_size + (n_concat - 2) * out_size, out_size, False)\n",
        "        self.conv = unetConv2(out_size*2, out_size, False)\n",
        "        if is_deconv:\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1)\n",
        "        else:\n",
        "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "        self.att = Attention_block(F_g=out_size,F_l=out_size,F_int=int(out_size/2))\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            if m.__class__.__name__.find('unetConv2') != -1: continue\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs0, *input):\n",
        "        # print(self.n_concat)\n",
        "        # print(input)\n",
        "        outputs0 = self.up(inputs0)\n",
        "        for i in range(len(input)):\n",
        "            outputs0 = torch.cat([outputs0, input[i]], 1)\n",
        "        return self.conv(outputs0)\n",
        "\n",
        "class Attention_block(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(Attention_block, self).__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        try: \n",
        "            psi = self.relu(g1+x1)\n",
        "        except:\n",
        "            psi = self.relu(F.pad(g1,(1,0,1,0)) + x1)\n",
        "        psi = self.psi(psi)\n",
        "        return x * psi\n",
        "    \n",
        "class unetUp_origin(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_deconv, n_concat=2):\n",
        "        super(unetUp_origin, self).__init__()\n",
        "        # self.conv = unetConv2(out_size*2, out_size, False)\n",
        "        if is_deconv:\n",
        "            self.conv = unetConv2(in_size + (n_concat - 2) * out_size, out_size, False)\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
        "        else:\n",
        "            self.conv = unetConv2(in_size + (n_concat - 2) * out_size, out_size, False)\n",
        "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "        self.att = Attention_block(F_g=out_size,F_l=out_size,F_int=int(out_size/2))\n",
        "        # initialise the blocks\n",
        "        for m in self.children():\n",
        "            if m.__class__.__name__.find('unetConv2') != -1: continue\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs0, *input):\n",
        "        x1 = self.up(inputs0)\n",
        "        for i in range(len(input)):\n",
        "            try:\n",
        "                try:\n",
        "                    x1 = self.att(x1,input[i])\n",
        "                except:\n",
        "                    x1 = x1\n",
        "                x1 = torch.cat((x1,input[i]),1)\n",
        "            except:\n",
        "                try:\n",
        "                    x1 = self.att(x1,input[i])\n",
        "                except:\n",
        "                    x1 = x1\n",
        "                x1 = torch.cat((F.pad(x1,(1,0,1,0)),input[i]),1)\n",
        "        return self.conv(x1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "075b5c9e",
      "metadata": {
        "id": "075b5c9e"
      },
      "outputs": [],
      "source": [
        "class UNet_2Plus(nn.Module):\n",
        "    def __init__(self, in_channels=1, n_classes=1, feature_scale=4, is_deconv=True, is_batchnorm=True, is_ds=True):\n",
        "        super(UNet_2Plus, self).__init__()\n",
        "        self.is_deconv = is_deconv\n",
        "        self.in_channels = in_channels\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "        self.is_ds = is_ds\n",
        "        self.feature_scale = feature_scale\n",
        "\n",
        "        filters = [64, 128, 256, 512, 1024]\n",
        "\n",
        "        # downsampling\n",
        "        self.conv00 = unetConv2(self.in_channels, filters[0], self.is_batchnorm)\n",
        "        self.maxpool0 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv10 = unetConv2(filters[0], filters[1], self.is_batchnorm)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv20 = unetConv2(filters[1], filters[2], self.is_batchnorm)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv30 = unetConv2(filters[2], filters[3], self.is_batchnorm)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv40 = unetConv2(filters[3], filters[4], self.is_batchnorm)\n",
        "\n",
        "\n",
        "        # upsampling\n",
        "        self.up_concat01 = unetUp_origin(filters[1], filters[0], self.is_deconv)\n",
        "        self.up_concat11 = unetUp_origin(filters[2], filters[1], self.is_deconv)\n",
        "        self.up_concat21 = unetUp_origin(filters[3], filters[2], self.is_deconv)\n",
        "        self.up_concat31 = unetUp_origin(filters[4], filters[3], self.is_deconv)\n",
        "\n",
        "        self.up_concat02 = unetUp_origin(filters[1], filters[0], self.is_deconv, 3)\n",
        "        self.up_concat12 = unetUp_origin(filters[2], filters[1], self.is_deconv, 3)\n",
        "        self.up_concat22 = unetUp_origin(filters[3], filters[2], self.is_deconv, 3)\n",
        "\n",
        "        self.up_concat03 = unetUp_origin(filters[1], filters[0], self.is_deconv, 4)\n",
        "        self.up_concat13 = unetUp_origin(filters[2], filters[1], self.is_deconv, 4)\n",
        "\n",
        "        self.up_concat04 = unetUp_origin(filters[1], filters[0], self.is_deconv, 5)\n",
        "\n",
        "        # final conv (without any concat)\n",
        "        self.final_1 = nn.Conv2d(filters[0], n_classes, 1)\n",
        "        self.final_2 = nn.Conv2d(filters[0], n_classes, 1)\n",
        "        self.final_3 = nn.Conv2d(filters[0], n_classes, 1)\n",
        "        self.final_4 = nn.Conv2d(filters[0], n_classes, 1)\n",
        "\n",
        "        # initialise weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init_weights(m, init_type='kaiming')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init_weights(m, init_type='kaiming')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # column : 0\n",
        "        X_00 = self.conv00(inputs)\n",
        "        maxpool0 = self.maxpool0(X_00)\n",
        "        X_10 = self.conv10(maxpool0)\n",
        "        maxpool1 = self.maxpool1(X_10)\n",
        "        X_20 = self.conv20(maxpool1)\n",
        "        maxpool2 = self.maxpool2(X_20)\n",
        "        X_30 = self.conv30(maxpool2)\n",
        "        maxpool3 = self.maxpool3(X_30)\n",
        "        X_40 = self.conv40(maxpool3)\n",
        "\n",
        "        # column : 1\n",
        "        X_01 = self.up_concat01(X_10, X_00)\n",
        "        X_11 = self.up_concat11(X_20, X_10)\n",
        "        X_21 = self.up_concat21(X_30, X_20)\n",
        "        X_31 = self.up_concat31(X_40, X_30)\n",
        "        # column : 2\n",
        "        X_02 = self.up_concat02(X_11, X_00, X_01)\n",
        "        X_12 = self.up_concat12(X_21, X_10, X_11)\n",
        "        X_22 = self.up_concat22(X_31, X_20, X_21)\n",
        "        # column : 3\n",
        "        X_03 = self.up_concat03(X_12, X_00, X_01, X_02)\n",
        "        X_13 = self.up_concat13(X_22, X_10, X_11, X_12)\n",
        "        # column : 4\n",
        "        X_04 = self.up_concat04(X_13, X_00, X_01, X_02, X_03)\n",
        "\n",
        "        # final layer\n",
        "        final_1 = self.final_1(X_01)\n",
        "        final_2 = self.final_2(X_02)\n",
        "        final_3 = self.final_3(X_03)\n",
        "        final_4 = self.final_4(X_04)\n",
        "\n",
        "        final = (final_1 + final_2 + final_3 + final_4) / 4\n",
        "\n",
        "        if self.is_ds:\n",
        "            return F.sigmoid(final)\n",
        "        else:\n",
        "            return F.sigmoid(final_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec0bcc7b",
      "metadata": {
        "id": "ec0bcc7b",
        "outputId": "43fbe11c-0d2f-43bb-d307-3ef13fb92974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Loaded to GPU\n"
          ]
        }
      ],
      "source": [
        "model = UNet_2Plus()\n",
        "model.to('cuda')\n",
        "print(\"Model Loaded to GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f087ee21",
      "metadata": {
        "id": "f087ee21"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9849b274",
      "metadata": {
        "id": "9849b274"
      },
      "outputs": [],
      "source": [
        "# calculates similarity index between predicted and actual segmentation\n",
        "def dice_index(y_pred, y_actual):\n",
        "    smooth = 0.000001\n",
        "    size_of_batch = y_pred.size(0)\n",
        "    \n",
        "    p1 = y_pred.view(size_of_batch, -1)\n",
        "    p2 = y_actual.view(size_of_batch, -1)\n",
        "    \n",
        "    intersection = (p1 * p2).sum()\n",
        "    \n",
        "    dice =  ((2.0 * intersection )+ smooth) / (p1.sum() + p2.sum() + smooth)\n",
        "    #dice.requires_grad = True\n",
        "    \n",
        "    return dice\n",
        "\n",
        "# calculate dice loss which will be later used in loss function calculation\n",
        "def dice_loss(y_predict, y_train): ## to add in bce looss\n",
        "    return 1 -(dice_index(y_predict, y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Only train the model if you want to retrain, screw down to load the pre-trained model"
      ],
      "metadata": {
        "id": "xsIAMUQ4lzr0"
      },
      "id": "xsIAMUQ4lzr0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3526d833",
      "metadata": {
        "scrolled": false,
        "id": "3526d833",
        "outputId": "b8bcf1e0-72c9-4cc3-a010-525bddb1430a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mmfs1/data/licds/.local/lib/python3.9/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  1 Batch:  0 Current Loss:  0.6528282165527344\n",
            "Epoch:  1 Batch:  50 Current Loss:  0.5423264503479004\n",
            "Epoch:  1 Batch:  100 Current Loss:  0.5444811582565308\n",
            "Epoch:  1 Batch:  150 Current Loss:  0.48045438528060913\n",
            "Epoch:  1 Batch:  200 Current Loss:  0.38965702056884766\n",
            "Epoch:  1 Batch:  250 Current Loss:  0.2999286353588104\n",
            "Epoch:  1 Batch:  300 Current Loss:  0.18294861912727356\n",
            "Epoch:  1 Batch:  350 Current Loss:  0.28020885586738586\n",
            "================================================================================\n",
            "Epoch 1 completed\n",
            "Average train loss is 0.414221759326756: \n",
            "Average validation loss is 0.2653943356871605\n",
            "================================================================================\n",
            "Epoch:  2 Batch:  0 Current Loss:  0.4685017466545105\n",
            "Epoch:  2 Batch:  50 Current Loss:  0.34712791442871094\n",
            "Epoch:  2 Batch:  100 Current Loss:  0.19392269849777222\n",
            "Epoch:  2 Batch:  150 Current Loss:  0.3692450523376465\n",
            "Epoch:  2 Batch:  200 Current Loss:  0.23630252480506897\n",
            "Epoch:  2 Batch:  250 Current Loss:  0.23198175430297852\n",
            "Epoch:  2 Batch:  300 Current Loss:  0.2026372253894806\n",
            "Epoch:  2 Batch:  350 Current Loss:  0.17415212094783783\n",
            "================================================================================\n",
            "Epoch 2 completed\n",
            "Average train loss is 0.2573664986528456: \n",
            "Average validation loss is 0.20243034943938254\n",
            "================================================================================\n",
            "Epoch:  3 Batch:  0 Current Loss:  0.4011584222316742\n",
            "Epoch:  3 Batch:  50 Current Loss:  0.3776334524154663\n",
            "Epoch:  3 Batch:  100 Current Loss:  0.11710446327924728\n",
            "Epoch:  3 Batch:  150 Current Loss:  0.2409619241952896\n",
            "Epoch:  3 Batch:  200 Current Loss:  0.23350779712200165\n",
            "Epoch:  3 Batch:  250 Current Loss:  0.21078304946422577\n",
            "Epoch:  3 Batch:  300 Current Loss:  0.11614677309989929\n",
            "Epoch:  3 Batch:  350 Current Loss:  0.12671378254890442\n",
            "================================================================================\n",
            "Epoch 3 completed\n",
            "Average train loss is 0.20476278403773904: \n",
            "Average validation loss is 0.16224733032286168\n",
            "================================================================================\n",
            "Epoch:  4 Batch:  0 Current Loss:  0.3041702210903168\n",
            "Epoch:  4 Batch:  50 Current Loss:  0.23749658465385437\n",
            "Epoch:  4 Batch:  100 Current Loss:  0.10747531056404114\n",
            "Epoch:  4 Batch:  150 Current Loss:  0.24259892106056213\n",
            "Epoch:  4 Batch:  200 Current Loss:  0.11873796582221985\n",
            "Epoch:  4 Batch:  250 Current Loss:  0.1543576568365097\n",
            "Epoch:  4 Batch:  300 Current Loss:  0.12477915734052658\n",
            "Epoch:  4 Batch:  350 Current Loss:  0.10954355448484421\n",
            "================================================================================\n",
            "Epoch 4 completed\n",
            "Average train loss is 0.1672146999463439: \n",
            "Average validation loss is 0.12877609744668006\n",
            "================================================================================\n",
            "Epoch:  5 Batch:  0 Current Loss:  0.29016047716140747\n",
            "Epoch:  5 Batch:  50 Current Loss:  0.21077418327331543\n",
            "Epoch:  5 Batch:  100 Current Loss:  0.0663147121667862\n",
            "Epoch:  5 Batch:  150 Current Loss:  0.24804723262786865\n",
            "Epoch:  5 Batch:  200 Current Loss:  0.06122642755508423\n",
            "Epoch:  5 Batch:  250 Current Loss:  0.12677565217018127\n",
            "Epoch:  5 Batch:  300 Current Loss:  0.11151659488677979\n",
            "Epoch:  5 Batch:  350 Current Loss:  0.07943779230117798\n",
            "================================================================================\n",
            "Epoch 5 completed\n",
            "Average train loss is 0.1446517219673842: \n",
            "Average validation loss is 0.11612998403608799\n",
            "================================================================================\n",
            "Epoch:  6 Batch:  0 Current Loss:  0.17350207269191742\n",
            "Epoch:  6 Batch:  50 Current Loss:  0.20294435322284698\n",
            "Epoch:  6 Batch:  100 Current Loss:  0.07773099094629288\n",
            "Epoch:  6 Batch:  150 Current Loss:  0.10347889363765717\n",
            "Epoch:  6 Batch:  200 Current Loss:  0.061669185757637024\n",
            "Epoch:  6 Batch:  250 Current Loss:  0.12336950749158859\n",
            "Epoch:  6 Batch:  300 Current Loss:  0.10836917161941528\n",
            "Epoch:  6 Batch:  350 Current Loss:  0.06239733099937439\n",
            "================================================================================\n",
            "Epoch 6 completed\n",
            "Average train loss is 0.12747603215277195: \n",
            "Average validation loss is 0.10410460334271193\n",
            "================================================================================\n",
            "Epoch:  7 Batch:  0 Current Loss:  0.1876416951417923\n",
            "Epoch:  7 Batch:  50 Current Loss:  0.16999784111976624\n",
            "Epoch:  7 Batch:  100 Current Loss:  0.058388471603393555\n",
            "Epoch:  7 Batch:  150 Current Loss:  0.1381576657295227\n",
            "Epoch:  7 Batch:  200 Current Loss:  0.05662072077393532\n",
            "Epoch:  7 Batch:  250 Current Loss:  0.09289807826280594\n",
            "Epoch:  7 Batch:  300 Current Loss:  0.0860448032617569\n",
            "Epoch:  7 Batch:  350 Current Loss:  0.09650591015815735\n",
            "================================================================================\n",
            "Epoch 7 completed\n",
            "Average train loss is 0.11843205858953297: \n",
            "Average validation loss is 0.10363704081624746\n",
            "================================================================================\n",
            "Epoch:  8 Batch:  0 Current Loss:  0.20005030930042267\n",
            "Epoch:  8 Batch:  50 Current Loss:  0.2014663815498352\n",
            "Epoch:  8 Batch:  100 Current Loss:  0.04720832034945488\n",
            "Epoch:  8 Batch:  150 Current Loss:  0.2079823911190033\n",
            "Epoch:  8 Batch:  200 Current Loss:  0.05887399986386299\n",
            "Epoch:  8 Batch:  250 Current Loss:  0.11414952576160431\n",
            "Epoch:  8 Batch:  300 Current Loss:  0.08227644115686417\n",
            "Epoch:  8 Batch:  350 Current Loss:  0.11219188570976257\n",
            "================================================================================\n",
            "Epoch 8 completed\n",
            "Average train loss is 0.10905798922292888: \n",
            "Average validation loss is 0.12475070226937532\n",
            "================================================================================\n",
            "Epoch:  9 Batch:  0 Current Loss:  0.14367613196372986\n",
            "Epoch:  9 Batch:  50 Current Loss:  0.12324750423431396\n",
            "Epoch:  9 Batch:  100 Current Loss:  0.0392286591231823\n",
            "Epoch:  9 Batch:  150 Current Loss:  0.11502595245838165\n",
            "Epoch:  9 Batch:  200 Current Loss:  0.0464710108935833\n",
            "Epoch:  9 Batch:  250 Current Loss:  0.10446661710739136\n",
            "Epoch:  9 Batch:  300 Current Loss:  0.07988572120666504\n",
            "Epoch:  9 Batch:  350 Current Loss:  0.07461272180080414\n",
            "================================================================================\n",
            "Epoch 9 completed\n",
            "Average train loss is 0.10155356298200786: \n",
            "Average validation loss is 0.09648332430049777\n",
            "================================================================================\n",
            "Epoch:  10 Batch:  0 Current Loss:  0.09806802868843079\n",
            "Epoch:  10 Batch:  50 Current Loss:  0.17512601613998413\n",
            "Epoch:  10 Batch:  100 Current Loss:  0.09101004898548126\n",
            "Epoch:  10 Batch:  150 Current Loss:  0.20716163516044617\n",
            "Epoch:  10 Batch:  200 Current Loss:  0.05253661796450615\n",
            "Epoch:  10 Batch:  250 Current Loss:  0.10191989690065384\n",
            "Epoch:  10 Batch:  300 Current Loss:  0.041489165276288986\n",
            "Epoch:  10 Batch:  350 Current Loss:  0.06206688657402992\n",
            "================================================================================\n",
            "Epoch 10 completed\n",
            "Average train loss is 0.09526259295642375: \n",
            "Average validation loss is 0.11438101720064879\n",
            "================================================================================\n",
            "Epoch:  11 Batch:  0 Current Loss:  0.1019519716501236\n",
            "Epoch:  11 Batch:  50 Current Loss:  0.1149941235780716\n",
            "Epoch:  11 Batch:  100 Current Loss:  0.04794003814458847\n",
            "Epoch:  11 Batch:  150 Current Loss:  0.09853419661521912\n",
            "Epoch:  11 Batch:  200 Current Loss:  0.04601700231432915\n",
            "Epoch:  11 Batch:  250 Current Loss:  0.08225443214178085\n",
            "Epoch:  11 Batch:  300 Current Loss:  0.06044549494981766\n",
            "Epoch:  11 Batch:  350 Current Loss:  0.07127124071121216\n",
            "================================================================================\n",
            "Epoch 11 completed\n",
            "Average train loss is 0.09344197869300842: \n",
            "Average validation loss is 0.1049667676910758\n",
            "================================================================================\n",
            "Epoch:  12 Batch:  0 Current Loss:  0.15349142253398895\n",
            "Epoch:  12 Batch:  50 Current Loss:  0.1290888488292694\n",
            "Epoch:  12 Batch:  100 Current Loss:  0.0588960200548172\n",
            "Epoch:  12 Batch:  150 Current Loss:  0.13793563842773438\n",
            "Epoch:  12 Batch:  200 Current Loss:  0.05582030117511749\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  12 Batch:  250 Current Loss:  0.08987139910459518\n",
            "Epoch:  12 Batch:  300 Current Loss:  0.05119908228516579\n",
            "Epoch:  12 Batch:  350 Current Loss:  0.10415156185626984\n",
            "================================================================================\n",
            "Epoch 12 completed\n",
            "Average train loss is 0.0927575525920838: \n",
            "Average validation loss is 0.09592734824866056\n",
            "================================================================================\n",
            "Epoch:  13 Batch:  0 Current Loss:  0.16575628519058228\n",
            "Epoch:  13 Batch:  50 Current Loss:  0.10569243878126144\n",
            "Epoch:  13 Batch:  100 Current Loss:  0.03272747993469238\n",
            "Epoch:  13 Batch:  150 Current Loss:  0.15392154455184937\n",
            "Epoch:  13 Batch:  200 Current Loss:  0.058711614459753036\n",
            "Epoch:  13 Batch:  250 Current Loss:  0.08751507848501205\n",
            "Epoch:  13 Batch:  300 Current Loss:  0.059106726199388504\n",
            "Epoch:  13 Batch:  350 Current Loss:  0.07492603361606598\n",
            "================================================================================\n",
            "Epoch 13 completed\n",
            "Average train loss is 0.08489546183031052: \n",
            "Average validation loss is 0.08450441593304277\n",
            "================================================================================\n",
            "Epoch:  14 Batch:  0 Current Loss:  0.14431172609329224\n",
            "Epoch:  14 Batch:  50 Current Loss:  0.09052687138319016\n",
            "Epoch:  14 Batch:  100 Current Loss:  0.04164740815758705\n",
            "Epoch:  14 Batch:  150 Current Loss:  0.11399562656879425\n",
            "Epoch:  14 Batch:  200 Current Loss:  0.05289306864142418\n",
            "Epoch:  14 Batch:  250 Current Loss:  0.12038905918598175\n",
            "Epoch:  14 Batch:  300 Current Loss:  0.045339494943618774\n",
            "Epoch:  14 Batch:  350 Current Loss:  0.054540228098630905\n",
            "================================================================================\n",
            "Epoch 14 completed\n",
            "Average train loss is 0.08217807503882796: \n",
            "Average validation loss is 0.0750068118236959\n",
            "================================================================================\n",
            "Epoch:  15 Batch:  0 Current Loss:  0.117762491106987\n",
            "Epoch:  15 Batch:  50 Current Loss:  0.12471945583820343\n",
            "Epoch:  15 Batch:  100 Current Loss:  0.03894972801208496\n",
            "Epoch:  15 Batch:  150 Current Loss:  0.10101795941591263\n",
            "Epoch:  15 Batch:  200 Current Loss:  0.04334942251443863\n",
            "Epoch:  15 Batch:  250 Current Loss:  0.07955344021320343\n",
            "Epoch:  15 Batch:  300 Current Loss:  0.056518204510211945\n",
            "Epoch:  15 Batch:  350 Current Loss:  0.05921303108334541\n",
            "================================================================================\n",
            "Epoch 15 completed\n",
            "Average train loss is 0.08481665129773319: \n",
            "Average validation loss is 0.1243797493726015\n",
            "================================================================================\n",
            "Epoch:  16 Batch:  0 Current Loss:  0.13908572494983673\n",
            "Epoch:  16 Batch:  50 Current Loss:  0.06378108263015747\n",
            "Epoch:  16 Batch:  100 Current Loss:  0.03564634919166565\n",
            "Epoch:  16 Batch:  150 Current Loss:  0.09488450735807419\n",
            "Epoch:  16 Batch:  200 Current Loss:  0.03996972739696503\n",
            "Epoch:  16 Batch:  250 Current Loss:  0.09121476113796234\n",
            "Epoch:  16 Batch:  300 Current Loss:  0.1448655128479004\n",
            "Epoch:  16 Batch:  350 Current Loss:  0.0715574324131012\n",
            "================================================================================\n",
            "Epoch 16 completed\n",
            "Average train loss is 0.08021914420649409: \n",
            "Average validation loss is 0.10224033903330565\n",
            "================================================================================\n",
            "Epoch:  17 Batch:  0 Current Loss:  0.1197139099240303\n",
            "Epoch:  17 Batch:  50 Current Loss:  0.11204981803894043\n",
            "Epoch:  17 Batch:  100 Current Loss:  0.03369292616844177\n",
            "Epoch:  17 Batch:  150 Current Loss:  0.07757264375686646\n",
            "Epoch:  17 Batch:  200 Current Loss:  0.04134929180145264\n",
            "Epoch:  17 Batch:  250 Current Loss:  0.0875353217124939\n",
            "Epoch:  17 Batch:  300 Current Loss:  0.04217584431171417\n",
            "Epoch:  17 Batch:  350 Current Loss:  0.07598161697387695\n",
            "================================================================================\n",
            "Epoch 17 completed\n",
            "Average train loss is 0.07595182282850146: \n",
            "Average validation loss is 0.1025381045974791\n",
            "================================================================================\n",
            "Epoch:  18 Batch:  0 Current Loss:  0.14210236072540283\n",
            "Epoch:  18 Batch:  50 Current Loss:  0.11247727274894714\n",
            "Epoch:  18 Batch:  100 Current Loss:  0.03962092101573944\n",
            "Epoch:  18 Batch:  150 Current Loss:  0.07635243982076645\n",
            "Epoch:  18 Batch:  200 Current Loss:  0.046372778713703156\n",
            "Epoch:  18 Batch:  250 Current Loss:  0.1063666045665741\n",
            "Epoch:  18 Batch:  300 Current Loss:  0.06883110851049423\n",
            "Epoch:  18 Batch:  350 Current Loss:  0.045584987848997116\n",
            "================================================================================\n",
            "Epoch 18 completed\n",
            "Average train loss is 0.07795037310104817: \n",
            "Average validation loss is 0.08540479930117727\n",
            "================================================================================\n",
            "Epoch:  19 Batch:  0 Current Loss:  0.09925244003534317\n",
            "Epoch:  19 Batch:  50 Current Loss:  0.15949738025665283\n",
            "Epoch:  19 Batch:  100 Current Loss:  0.03175722807645798\n",
            "Epoch:  19 Batch:  150 Current Loss:  0.09814617037773132\n",
            "Epoch:  19 Batch:  200 Current Loss:  0.042599767446517944\n",
            "Epoch:  19 Batch:  250 Current Loss:  0.07825102657079697\n",
            "Epoch:  19 Batch:  300 Current Loss:  0.048209913074970245\n",
            "Epoch:  19 Batch:  350 Current Loss:  0.055703699588775635\n",
            "================================================================================\n",
            "Epoch 19 completed\n",
            "Average train loss is 0.07404931043274701: \n",
            "Average validation loss is 0.10428227756172419\n",
            "================================================================================\n",
            "Epoch:  20 Batch:  0 Current Loss:  0.11257240176200867\n",
            "Epoch:  20 Batch:  50 Current Loss:  0.15857231616973877\n",
            "Epoch:  20 Batch:  100 Current Loss:  0.03474394977092743\n",
            "Epoch:  20 Batch:  150 Current Loss:  0.07311808317899704\n",
            "Epoch:  20 Batch:  200 Current Loss:  0.04659244045615196\n",
            "Epoch:  20 Batch:  250 Current Loss:  0.0801236480474472\n",
            "Epoch:  20 Batch:  300 Current Loss:  0.057755667716264725\n",
            "Epoch:  20 Batch:  350 Current Loss:  0.05691685900092125\n",
            "================================================================================\n",
            "Epoch 20 completed\n",
            "Average train loss is 0.0714357578009367: \n",
            "Average validation loss is 0.09251489505171775\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "epochs = 20 \n",
        "\n",
        "train_running_loss_history = []\n",
        "validation_running_loss_history =[]\n",
        "\n",
        "for e in range(epochs):\n",
        "    train_running_loss = 0.0\n",
        "    validation_running_loss = 0.0  \n",
        "    model.train()\n",
        "    for ith_batch, sample_batched in enumerate(train_loader):\n",
        "        X_train = sample_batched['image'].cuda()\n",
        "        y_train = sample_batched['annotation'].to(\"cuda:0\")\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_train)\n",
        "        loss = 0.30 * dice_loss(y_pred, y_train) +  0.70 * criterion(y_pred, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if ith_batch % 50 == 0:\n",
        "            print('Epoch: ', e + 1, 'Batch: ', ith_batch, 'Current Loss: ', loss.item())\n",
        "        train_running_loss += loss.item()\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for ith_batch, sample_batched in enumerate(validation_loader):\n",
        "                X_val = sample_batched['image'].cuda()\n",
        "                y_val = sample_batched['annotation'].to(\"cuda:0\")\n",
        "                y_out = model(X_val)\n",
        "                out_val = (y_out + 0.5).int().float()\n",
        "                val_loss = 0.3 * dice_loss(out_val, y_val)  + 0.7 * criterion(y_out, y_val)\n",
        "                validation_running_loss += val_loss.item()\n",
        "            print(\"================================================================================\")\n",
        "            print(\"Epoch {} completed\".format(e + 1))\n",
        "\n",
        "            train_epoch_loss = train_running_loss / len(train_loader)\n",
        "            validation_epoch_loss = validation_running_loss / len(validation_loader)\n",
        "\n",
        "            print(\"Average train loss is {}: \".format(train_epoch_loss))\n",
        "            print(\"Average validation loss is {}\".format(validation_epoch_loss))\n",
        "            print(\"================================================================================\")\n",
        "            train_running_loss_history.append(train_epoch_loss)\n",
        "            validation_running_loss_history.append(validation_epoch_loss)\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e50f178",
      "metadata": {
        "id": "5e50f178",
        "outputId": "1e7b0ef0-ac53-4983-f59d-497193fa7aaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x155529ac1e50>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_running_loss_history, label = 'Train Loss')\n",
        "plt.plot(validation_running_loss_history, label = 'Validation Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf76e565",
      "metadata": {
        "id": "bf76e565",
        "outputId": "ab78b12b-3776-4069-9a58-97f4a3d2f6cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 572, 572])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()\n",
        "X_train.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc3976ab",
      "metadata": {
        "id": "fc3976ab"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'savedmodel/AttentionUNet++.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You can load a model instead"
      ],
      "metadata": {
        "id": "WSNPYlvCl4v7"
      },
      "id": "WSNPYlvCl4v7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe50d1b",
      "metadata": {
        "id": "abe50d1b"
      },
      "outputs": [],
      "source": [
        "def avg_dice_index(dataloader):\n",
        "    dice = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for ith_batch, sample_batched in enumerate(dataloader):\n",
        "            X_train = sample_batched['image'].to('cuda')\n",
        "            y_train = sample_batched['annotation'].to('cuda')\n",
        "            y_predict = (model(X_train) + 0.5).int().float()\n",
        "            dice += dice_index(y_predict, y_train)\n",
        "    avg_dice = dice / len(dataloader)\n",
        "    return avg_dice.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4558cda4",
      "metadata": {
        "id": "4558cda4",
        "outputId": "b8b6736f-746c-4dec-e4f9-bb7298ab37a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9360518455505371"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "avg_dice_index(validation_loader)    #### Dice index of validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "175c35d2",
      "metadata": {
        "id": "175c35d2",
        "outputId": "c00a7737-bf5b-45d4-9ba9-dfcc82812538"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9310593008995056"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "avg_dice_index(train_loader)    #### Dice index of train data"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}